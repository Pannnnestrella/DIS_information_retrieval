{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.15","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":85316,"databundleVersionId":9635715,"sourceType":"competition"},{"sourceId":9761079,"sourceType":"datasetVersion","datasetId":5977362},{"sourceId":9818095,"sourceType":"datasetVersion","datasetId":6019655}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install konlpy\n!pip install nltk\n!pip install faiss-cpu\n!pip install -U FlagEmbedding\n!pip install peft","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-11-08T17:38:21.294855Z","iopub.execute_input":"2024-11-08T17:38:21.295218Z","iopub.status.idle":"2024-11-08T17:39:09.859284Z","shell.execute_reply.started":"2024-11-08T17:38:21.295190Z","shell.execute_reply":"2024-11-08T17:39:09.858398Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting konlpy\n  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting lxml>=4.1.0\n  Downloading lxml-5.3.0-cp310-cp310-manylinux_2_28_x86_64.whl (5.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m78.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hCollecting JPype1>=0.7.0\n  Downloading JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (488 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m488.6/488.6 kB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/site-packages (from konlpy) (1.26.4)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/site-packages (from JPype1>=0.7.0->konlpy) (24.1)\nInstalling collected packages: lxml, JPype1, konlpy\nSuccessfully installed JPype1-1.5.0 konlpy-0.6.0 lxml-5.3.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\nCollecting nltk\n  Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/site-packages (from nltk) (8.1.7)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/site-packages (from nltk) (4.66.5)\nRequirement already satisfied: joblib in /usr/local/lib/python3.10/site-packages (from nltk) (1.4.2)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/site-packages (from nltk) (2024.9.11)\nInstalling collected packages: nltk\nSuccessfully installed nltk-3.9.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\nCollecting faiss-cpu\n  Downloading faiss_cpu-1.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/site-packages (from faiss-cpu) (24.1)\nRequirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.10/site-packages (from faiss-cpu) (1.26.4)\nInstalling collected packages: faiss-cpu\nSuccessfully installed faiss-cpu-1.9.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\nCollecting FlagEmbedding\n  Downloading FlagEmbedding-1.3.2.tar.gz (177 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.8/177.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/site-packages (from FlagEmbedding) (2.4.0)\nCollecting transformers==4.44.2\n  Downloading transformers-4.44.2-py3-none-any.whl (9.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting datasets==2.19.0\n  Downloading datasets-2.19.0-py3-none-any.whl (542 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: accelerate>=0.20.1 in /usr/local/lib/python3.10/site-packages (from FlagEmbedding) (0.34.2)\nCollecting sentence_transformers\n  Downloading sentence_transformers-3.2.1-py3-none-any.whl (255 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m255.8/255.8 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting peft\n  Downloading peft-0.13.2-py3-none-any.whl (320 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting ir-datasets\n  Downloading ir_datasets-0.5.9-py3-none-any.whl (347 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.9/347.9 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hCollecting sentencepiece\n  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.10/site-packages (from FlagEmbedding) (3.20.3)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/site-packages (from datasets==2.19.0->FlagEmbedding) (4.66.5)\nCollecting dill<0.3.9,>=0.3.0\n  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting fsspec[http]<=2024.3.1,>=2023.1.0\n  Downloading fsspec-2024.3.1-py3-none-any.whl (171 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.0/172.0 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/site-packages (from datasets==2.19.0->FlagEmbedding) (6.0.2)\nRequirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/site-packages (from datasets==2.19.0->FlagEmbedding) (0.25.1)\nCollecting pyarrow-hotfix\n  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/site-packages (from datasets==2.19.0->FlagEmbedding) (1.26.4)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/site-packages (from datasets==2.19.0->FlagEmbedding) (2.2.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from datasets==2.19.0->FlagEmbedding) (3.16.1)\nCollecting multiprocess\n  Downloading multiprocess-0.70.17-py310-none-any.whl (134 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting aiohttp\n  Downloading aiohttp-3.10.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting xxhash\n  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/site-packages (from datasets==2.19.0->FlagEmbedding) (17.0.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/site-packages (from datasets==2.19.0->FlagEmbedding) (24.1)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/site-packages (from datasets==2.19.0->FlagEmbedding) (2.32.3)\nCollecting tokenizers<0.20,>=0.19\n  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/site-packages (from transformers==4.44.2->FlagEmbedding) (0.4.5)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/site-packages (from transformers==4.44.2->FlagEmbedding) (2024.9.11)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/site-packages (from accelerate>=0.20.1->FlagEmbedding) (6.0.0)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/site-packages (from torch>=1.6.0->FlagEmbedding) (3.1.4)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/site-packages (from torch>=1.6.0->FlagEmbedding) (3.3)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/site-packages (from torch>=1.6.0->FlagEmbedding) (2024.9.0)\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/site-packages (from torch>=1.6.0->FlagEmbedding) (12.1.3.1)\nRequirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.10/site-packages (from torch>=1.6.0->FlagEmbedding) (3.0.0)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/site-packages (from torch>=1.6.0->FlagEmbedding) (10.3.2.106)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/site-packages (from torch>=1.6.0->FlagEmbedding) (11.4.5.107)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/site-packages (from torch>=1.6.0->FlagEmbedding) (1.13.3)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/site-packages (from torch>=1.6.0->FlagEmbedding) (12.1.105)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/site-packages (from torch>=1.6.0->FlagEmbedding) (11.0.2.54)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/site-packages (from torch>=1.6.0->FlagEmbedding) (4.12.2)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/site-packages (from torch>=1.6.0->FlagEmbedding) (12.1.105)\nRequirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/site-packages (from torch>=1.6.0->FlagEmbedding) (2.20.5)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/site-packages (from torch>=1.6.0->FlagEmbedding) (9.1.0.70)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/site-packages (from torch>=1.6.0->FlagEmbedding) (12.1.0.106)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/site-packages (from torch>=1.6.0->FlagEmbedding) (12.1.105)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/site-packages (from torch>=1.6.0->FlagEmbedding) (12.1.105)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.6.0->FlagEmbedding) (12.6.68)\nCollecting unlzw3>=0.2.1\n  Downloading unlzw3-0.2.2-py3-none-any.whl (6.1 kB)\nCollecting warc3-wet>=0.2.3\n  Downloading warc3_wet-0.2.5-py3-none-any.whl (18 kB)\nCollecting trec-car-tools>=2.5.4\n  Downloading trec_car_tools-2.6-py3-none-any.whl (8.4 kB)\nCollecting warc3-wet-clueweb09>=0.2.5\n  Downloading warc3-wet-clueweb09-0.2.5.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting zlib-state>=0.1.3\n  Downloading zlib_state-0.1.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21 kB)\nRequirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.10/site-packages (from ir-datasets->FlagEmbedding) (4.12.3)\nCollecting lz4>=3.1.10\n  Downloading lz4-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting ijson>=3.1.3\n  Downloading ijson-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: lxml>=4.5.2 in /usr/local/lib/python3.10/site-packages (from ir-datasets->FlagEmbedding) (5.3.0)\nCollecting inscriptis>=2.2.0\n  Downloading inscriptis-2.5.0-py3-none-any.whl (45 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.4/45.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/site-packages (from sentence_transformers->FlagEmbedding) (1.5.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/site-packages (from sentence_transformers->FlagEmbedding) (1.14.1)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.10/site-packages (from sentence_transformers->FlagEmbedding) (10.4.0)\nRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/site-packages (from beautifulsoup4>=4.4.1->ir-datasets->FlagEmbedding) (2.6)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets==2.19.0->FlagEmbedding) (24.2.0)\nCollecting multidict<7.0,>=4.5\n  Downloading multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.6/124.6 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting frozenlist>=1.1.1\n  Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (241 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.9/241.9 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting async-timeout<5.0,>=4.0\n  Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\nCollecting aiohappyeyeballs>=2.3.0\n  Downloading aiohappyeyeballs-2.4.3-py3-none-any.whl (14 kB)\nCollecting yarl<2.0,>=1.12.0\n  Downloading yarl-1.17.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.7/318.7 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting aiosignal>=1.1.2\n  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.19.0->FlagEmbedding) (2024.8.30)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.19.0->FlagEmbedding) (2.2.3)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.19.0->FlagEmbedding) (3.10)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.19.0->FlagEmbedding) (3.3.2)\nCollecting cbor>=1.0.0\n  Downloading cbor-1.0.0.tar.gz (20 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->FlagEmbedding) (2.1.5)\nCollecting multiprocess\n  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/site-packages (from pandas->datasets==2.19.0->FlagEmbedding) (2024.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/site-packages (from pandas->datasets==2.19.0->FlagEmbedding) (2024.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/site-packages (from pandas->datasets==2.19.0->FlagEmbedding) (2.9.0.post0)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/site-packages (from scikit-learn->sentence_transformers->FlagEmbedding) (3.5.0)\nRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/site-packages (from scikit-learn->sentence_transformers->FlagEmbedding) (1.4.2)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/site-packages (from sympy->torch>=1.6.0->FlagEmbedding) (1.3.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.19.0->FlagEmbedding) (1.16.0)\nCollecting propcache>=0.2.0\n  Downloading propcache-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (208 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.9/208.9 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: FlagEmbedding, warc3-wet-clueweb09, cbor\n  Building wheel for FlagEmbedding (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for FlagEmbedding: filename=FlagEmbedding-1.3.2-py3-none-any.whl size=239842 sha256=25f40fe3c0f2f921df6a4db9a4e73d2bab6eca32a79c0026bc1bbc1d3a13b298\n  Stored in directory: /root/.cache/pip/wheels/0c/c3/74/4211099081763eb72f104c712a5d01560ec2d136970782080c\n  Building wheel for warc3-wet-clueweb09 (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for warc3-wet-clueweb09: filename=warc3_wet_clueweb09-0.2.5-py3-none-any.whl size=18919 sha256=b1abb4370b646d01130ea0cf8d78672b9076e3e3f3f071d7cfb8e3f395ec3643\n  Stored in directory: /root/.cache/pip/wheels/1a/d7/91/7ffb991df87e62355d945745035470ba2616aa3d83a250b5f9\n  Building wheel for cbor (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for cbor: filename=cbor-1.0.0-cp310-cp310-linux_x86_64.whl size=53023 sha256=8cc57dbcdf1b12f9af6446927adc3be1b2f2187f5bc07e7642dae0c66cc2e250\n  Stored in directory: /root/.cache/pip/wheels/85/df/c9/b39e40eccaf76dbd218556639a6dc81562226f4c6a64902c85\nSuccessfully built FlagEmbedding warc3-wet-clueweb09 cbor\nInstalling collected packages: warc3-wet-clueweb09, warc3-wet, sentencepiece, ijson, cbor, zlib-state, xxhash, unlzw3, trec-car-tools, pyarrow-hotfix, propcache, multidict, lz4, fsspec, frozenlist, dill, async-timeout, aiohappyeyeballs, yarl, multiprocess, inscriptis, aiosignal, tokenizers, ir-datasets, aiohttp, transformers, sentence_transformers, peft, datasets, FlagEmbedding\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2024.9.0\n    Uninstalling fsspec-2024.9.0:\n      Successfully uninstalled fsspec-2024.9.0\n  Attempting uninstall: dill\n    Found existing installation: dill 0.3.9\n    Uninstalling dill-0.3.9:\n      Successfully uninstalled dill-0.3.9\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.20.0\n    Uninstalling tokenizers-0.20.0:\n      Successfully uninstalled tokenizers-0.20.0\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.45.1\n    Uninstalling transformers-4.45.1:\n      Successfully uninstalled transformers-4.45.1\nSuccessfully installed FlagEmbedding-1.3.2 aiohappyeyeballs-2.4.3 aiohttp-3.10.10 aiosignal-1.3.1 async-timeout-4.0.3 cbor-1.0.0 datasets-2.19.0 dill-0.3.8 frozenlist-1.5.0 fsspec-2024.3.1 ijson-3.3.0 inscriptis-2.5.0 ir-datasets-0.5.9 lz4-4.3.3 multidict-6.1.0 multiprocess-0.70.16 peft-0.13.2 propcache-0.2.0 pyarrow-hotfix-0.6 sentence_transformers-3.2.1 sentencepiece-0.2.0 tokenizers-0.19.1 transformers-4.44.2 trec-car-tools-2.6 unlzw3-0.2.2 warc3-wet-0.2.5 warc3-wet-clueweb09-0.2.5 xxhash-3.5.0 yarl-1.17.1 zlib-state-0.1.9\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\nRequirement already satisfied: peft in /usr/local/lib/python3.10/site-packages (0.13.2)\nRequirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/site-packages (from peft) (2.4.0)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.10/site-packages (from peft) (0.4.5)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/site-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/site-packages (from peft) (24.1)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/site-packages (from peft) (4.66.5)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.10/site-packages (from peft) (0.25.1)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/site-packages (from peft) (6.0.2)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/site-packages (from peft) (4.44.2)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/site-packages (from peft) (6.0.0)\nRequirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/site-packages (from peft) (0.34.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.16.1)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.3.1)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.3)\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.1.3.1)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (10.3.2.106)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.4)\nRequirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.0.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.1.105)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (11.0.2.54)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.1.105)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (11.4.5.107)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (9.1.0.70)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.1.0.106)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.1.105)\nRequirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (2.20.5)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.1.105)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->peft) (12.6.68)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/site-packages (from transformers->peft) (0.19.1)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/site-packages (from transformers->peft) (2024.9.11)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.10)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.8.30)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2.2.3)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"import re\nimport json\nimport numpy as np\nimport pandas as pd\nimport torch\nimport json\nfrom collections import defaultdict\nfrom tqdm import tqdm\nimport math\nfrom six import iteritems\nfrom six.moves import range\nimport heapq\nimport pickle\nfrom collections.abc import Iterable\nfrom collections import Counter\nimport scipy.sparse as sp\nfrom scipy.sparse import csr_matrix\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem import ISRIStemmer  # For Arabic stemming\nfrom konlpy.tag import Okt  # For Korean text processing\nimport nltk\nnltk.download('stopwords')\n\nimport faiss\nfrom FlagEmbedding import FlagModel","metadata":{"execution":{"iopub.status.busy":"2024-11-08T17:39:09.860990Z","iopub.execute_input":"2024-11-08T17:39:09.861253Z","iopub.status.idle":"2024-11-08T17:39:54.443278Z","shell.execute_reply.started":"2024-11-08T17:39:09.861225Z","shell.execute_reply":"2024-11-08T17:39:54.442578Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n/usr/local/lib/python3.10/site-packages/torch_xla/__init__.py:202: UserWarning: `tensorflow` can conflict with `torch-xla`. Prefer `tensorflow-cpu` when using PyTorch/XLA. To silence this warning, `pip uninstall -y tensorflow && pip install tensorflow-cpu`. If you are in a notebook environment such as Colab or Kaggle, restart your notebook runtime afterwards.\n  warnings.warn(\nWARNING: Logging before InitGoogle() is written to STDERR\nE0000 00:00:1731087586.740099      13 common_lib.cc:798] Could not set metric server port: INVALID_ARGUMENT: Could not find SliceBuilder port 8471 in any of the 0 ports provided in `tpu_process_addresses`=\"local\"\n=== Source Location Trace: === \nlearning/45eac/tfrc/runtime/common_lib.cc:479\nE1108 17:39:46.786718874     353 oauth2_credentials.cc:238]            oauth_fetch: UNKNOWN:C-ares status is not ARES_SUCCESS qtype=A name=metadata.google.internal. is_balancer=0: Domain name not found {created_time:\"2024-11-08T17:39:46.786700791+00:00\", grpc_status:2}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Load and process test data","metadata":{}},{"cell_type":"code","source":"corpus_file_path = '/kaggle/input/dis-project-1-document-retrieval/corpus.json/corpus.json'\n\n# Load the corpus JSON file\nwith open(corpus_file_path, 'r') as f:\n    corpus = json.load(f)\n    \ndocids = [doc[\"docid\"] for doc in corpus]\ndocids[0]","metadata":{"execution":{"iopub.status.busy":"2024-11-08T17:39:54.444123Z","iopub.execute_input":"2024-11-08T17:39:54.444609Z","iopub.status.idle":"2024-11-08T17:41:09.509292Z","shell.execute_reply.started":"2024-11-08T17:39:54.444580Z","shell.execute_reply":"2024-11-08T17:41:09.508517Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"'doc-en-9633'"},"metadata":{}}]},{"cell_type":"code","source":"# Load the test queries\ntest_path = '/kaggle/input/dis-project-1-document-retrieval/test.csv'\ntest_df = pd.read_csv(test_path)\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-11-08T17:41:09.511298Z","iopub.execute_input":"2024-11-08T17:41:09.511648Z","iopub.status.idle":"2024-11-08T17:41:09.553010Z","shell.execute_reply.started":"2024-11-08T17:41:09.511618Z","shell.execute_reply":"2024-11-08T17:41:09.552358Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"   id query_id                                              query lang\n0   0   q-en-0  What organization proposed listing PFOA under ...   en\n1   1   q-en-2      What type of coating do ZM1130 - ZM1132 have?   en\n2   2   q-en-4  What year did Deutsche Bank sell its stake in ...   en\n3   3   q-en-5  Who expressed exasperation when Raphael and Mo...   en\n4   4   q-en-7  Who commissioned Amy Beach to compose a choral...   en","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>query_id</th>\n      <th>query</th>\n      <th>lang</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>q-en-0</td>\n      <td>What organization proposed listing PFOA under ...</td>\n      <td>en</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>q-en-2</td>\n      <td>What type of coating do ZM1130 - ZM1132 have?</td>\n      <td>en</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>q-en-4</td>\n      <td>What year did Deutsche Bank sell its stake in ...</td>\n      <td>en</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>q-en-5</td>\n      <td>Who expressed exasperation when Raphael and Mo...</td>\n      <td>en</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>q-en-7</td>\n      <td>Who commissioned Amy Beach to compose a choral...</td>\n      <td>en</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Initialize stop words and stemmers for each language\nstop_words = {\n    'en': set(stopwords.words('english')),\n    'fr': set(stopwords.words('french')),\n    'de': set(stopwords.words('german')),\n    'es': set(stopwords.words('spanish')),\n    'it': set(stopwords.words('italian')),\n    'ar': set(stopwords.words('arabic')),\n    'ko': set()  # Korean stop words can be manually added if needed\n}\n\nstemmers = {\n    'en': SnowballStemmer('english'),\n    'fr': SnowballStemmer('french'),\n    'de': SnowballStemmer('german'),\n    'es': SnowballStemmer('spanish'),\n    'it': SnowballStemmer('italian'),\n    'ar': ISRIStemmer(),  # Arabic stemmer from NLTK\n#     'ko': Okt()  # Use Korean Okt tokenizer for morphological analysis\n}\n\n\ndef preprocess(text, lang):\n    \"\"\"Preprocesses a single text entry by removing URLs, punctuation, stop words, and applying stemming.\n    \n    Args:\n        text (str): The text to preprocess.\n        lang (str): The language code for the text.\n    \n    Returns:\n        str: The preprocessed text.\n    \"\"\"\n    stemmer = stemmers.get(lang)\n    stop_words_lang = stop_words.get(lang, set())\n\n    # Remove URLs and punctuation\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n    text = re.sub(r'[^\\w\\s]', ' ', text)\n    \n    # Tokenize and preprocess based on language\n    words = text.split()\n    if lang == 'ko':\n        # Special handling for Korean\n#         words = [word for word, tag in stemmer.pos(text) if word not in stop_words_lang]\n        words = words\n    else:\n        # Apply stop word removal and stemming\n        words = [stemmer.stem(word.lower()) if lang != 'ar' else stemmer.stem(word) \n                 for word in words if word.lower() not in stop_words_lang]\n    \n    return ' '.join(words)","metadata":{"execution":{"iopub.status.busy":"2024-11-08T17:41:09.554039Z","iopub.execute_input":"2024-11-08T17:41:09.554354Z","iopub.status.idle":"2024-11-08T17:41:09.573023Z","shell.execute_reply.started":"2024-11-08T17:41:09.554307Z","shell.execute_reply":"2024-11-08T17:41:09.572325Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"preprocessed_queries = []\nfor index, document in test_df.iterrows():\n    preprocessed_text = preprocess(document['query'], document['lang'])\n\n    # Add preprocessed text to the corpus with docid and lang\n    preprocessed_queries.append(preprocessed_text)","metadata":{"execution":{"iopub.status.busy":"2024-11-08T17:41:09.574041Z","iopub.execute_input":"2024-11-08T17:41:09.574351Z","iopub.status.idle":"2024-11-08T17:41:09.995819Z","shell.execute_reply.started":"2024-11-08T17:41:09.574283Z","shell.execute_reply":"2024-11-08T17:41:09.995074Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## BM 25 retrieval","metadata":{}},{"cell_type":"code","source":"PARAM_K1 = 1.5\nPARAM_B = 0.75\nEPSILON = 0.25\n\nclass FastBM25:\n    \"\"\"Implementation of Best Matching 25 ranking function.\"\"\"\n    \n    def __init__(self, corpus=None, load_path=None):\n        \"\"\"\n        Parameters\n        ----------\n        corpus : list of list of str\n            Given corpus.\n        load_path : str, optional\n            Path to load precomputed data.\n        \"\"\"\n        if load_path:\n            self.load(load_path)\n        elif corpus:\n            self.corpus_size = len(corpus)\n            self.avgdl = 0\n            self.doc_freqs = []\n            self.idf = {}\n            self.doc_len = {}\n            self._initialize(corpus)\n            self.corpus = corpus\n            self.get_score_by_reversed_index_all_documents(corpus)\n        else:\n            raise ValueError(\"Either corpus or load_path must be provided\")\n\n    def _initialize(self, corpus):\n        \"\"\"Calculates frequencies of terms in documents and in corpus. Also computes inverse document frequencies.\"\"\"\n        nd = {}  # word -> number of documents with word\n        num_doc = 0\n        for j, document in enumerate(tqdm(corpus, desc=\"Initializing corpus\")):\n            self.doc_len[j] = len(document)\n            num_doc += len(document)\n\n            frequencies = {}\n            for word in document:\n                frequencies[word] = frequencies.get(word, 0) + 1\n            self.doc_freqs.append(frequencies)\n\n            for word in frequencies:\n                nd[word] = nd.get(word, 0) + 1\n\n        self.avgdl = float(num_doc) / self.corpus_size\n\n        idf_sum = 0\n        negative_idfs = []\n        self.nd = nd\n        for word, freq in nd.items():\n            idf = math.log(self.corpus_size - freq + 0.5) - math.log(freq + 0.5)\n            self.idf[word] = idf\n            idf_sum += idf\n            if idf < 0:\n                negative_idfs.append(word)\n        \n        self.average_idf = float(idf_sum) / len(self.idf)\n        eps = EPSILON * self.average_idf\n        for word in negative_idfs:\n            self.idf[word] = eps\n\n    def get_score_by_reversed_index_all_documents(self, corpus):\n        \"\"\"\n        Build reverted index for documents like {word:{index:grades}}\n        \"\"\"\n        document_score = {}\n        for index, document in enumerate(tqdm(corpus, desc=\"Building reversed index\")):\n            q_id = index\n            doc_freqs = self.doc_freqs[index]\n            for word in document:\n                if word not in doc_freqs:\n                    continue\n                score = (self.idf[word] * doc_freqs[word] * (PARAM_K1 + 1)\n                          / (doc_freqs[word] + PARAM_K1 * (1 - PARAM_B + PARAM_B * self.doc_len[index] / self.avgdl)))\n                if word not in document_score:\n                    document_score[word] = {q_id: round(score, 2)}\n                else:\n                    document_score[word].update({q_id: round(score, 2)})\n        self.document_score = document_score\n\n    def top_k_sentence(self, document, k=1):\n        \"\"\"\n        document: Iterable, to be retrieved\n        Returns\n        -------\n        float\n            List of [(nearest sentence,index,score)].\n        \"\"\"\n        assert isinstance(document, Iterable), 'document is not iterable'\n        score_overall = {}\n        for word in document:\n            if word not in self.document_score:\n                continue\n            for key, value in self.document_score[word].items():\n                score_overall[key] = score_overall.get(key, 0) + value\n        k_keys_sorted = heapq.nlargest(k, score_overall, key=score_overall.get)\n        return [(item, score_overall.get(item, None)) for item in k_keys_sorted]\n\n    def save(self, path):\n        \"\"\"Save the BM25 model to a file.\"\"\"\n        data = {\n            'corpus_size': self.corpus_size,\n            'avgdl': self.avgdl,\n            'doc_freqs': self.doc_freqs,\n            'idf': self.idf,\n            'doc_len': self.doc_len,\n            'document_score': self.document_score\n        }\n        with open(path, 'wb') as file:\n            pickle.dump(data, file)\n        print(f\"Model saved to {path}\")\n\n    def load(self, path):\n        \"\"\"Load the BM25 model from a file.\"\"\"\n        with open(path, 'rb') as file:\n            data = pickle.load(file)\n        self.corpus_size = data['corpus_size']\n        self.avgdl = data['avgdl']\n        self.doc_freqs = data['doc_freqs']\n        self.idf = data['idf']\n        self.doc_len = data['doc_len']\n        self.document_score = data['document_score']\n        print(f\"Model loaded from {path}\")","metadata":{"execution":{"iopub.status.busy":"2024-11-08T17:41:09.996846Z","iopub.execute_input":"2024-11-08T17:41:09.997085Z","iopub.status.idle":"2024-11-08T17:41:10.013026Z","shell.execute_reply.started":"2024-11-08T17:41:09.997060Z","shell.execute_reply":"2024-11-08T17:41:10.012343Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"bm25_model = FastBM25(load_path='/kaggle/input/bm25-model/bm25_model.pkl')","metadata":{"execution":{"iopub.status.busy":"2024-11-08T17:41:10.014181Z","iopub.execute_input":"2024-11-08T17:41:10.014817Z","iopub.status.idle":"2024-11-08T17:43:15.043984Z","shell.execute_reply.started":"2024-11-08T17:41:10.014780Z","shell.execute_reply":"2024-11-08T17:43:15.043159Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Model loaded from /kaggle/input/bm25-model/bm25_model.pkl\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\ntopk_docs = []\nfor query in tqdm(preprocessed_queries):\n    query = query.split()\n    top_k_results = bm25_model.top_k_sentence(query, k=10)\n    topk_docs.append([docids[result[0]] for result in top_k_results]) ","metadata":{"execution":{"iopub.status.busy":"2024-11-08T17:43:15.045076Z","iopub.execute_input":"2024-11-08T17:43:15.045377Z","iopub.status.idle":"2024-11-08T17:44:55.761708Z","shell.execute_reply.started":"2024-11-08T17:43:15.045347Z","shell.execute_reply":"2024-11-08T17:44:55.760994Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"100%|██████████| 2000/2000 [01:40<00:00, 19.86it/s] ","output_type":"stream"},{"name":"stdout","text":"CPU times: user 1min 40s, sys: 270 ms, total: 1min 41s\nWall time: 1min 40s\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"top_bm25 = pd.DataFrame({\n    'id': [i for i in range(len(topk_docs))],\n    'docids': topk_docs\n})\ntop_bm25.head()","metadata":{"execution":{"iopub.status.busy":"2024-11-08T17:58:38.346923Z","iopub.execute_input":"2024-11-08T17:58:38.347502Z","iopub.status.idle":"2024-11-08T17:58:38.360826Z","shell.execute_reply.started":"2024-11-08T17:58:38.347459Z","shell.execute_reply":"2024-11-08T17:58:38.360006Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"   id                                             docids\n0   0  [doc-en-0, doc-en-14117, doc-en-794977, doc-en...\n1   1  [doc-en-16, doc-en-773190, doc-en-369832, doc-...\n2   2  [doc-en-24920, doc-en-32, doc-en-420197, doc-e...\n3   3  [doc-en-814022, doc-en-40, doc-en-535898, doc-...\n4   4  [doc-en-701524, doc-en-56, doc-en-11186, doc-e...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>docids</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>[doc-en-0, doc-en-14117, doc-en-794977, doc-en...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>[doc-en-16, doc-en-773190, doc-en-369832, doc-...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>[doc-en-24920, doc-en-32, doc-en-420197, doc-e...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>[doc-en-814022, doc-en-40, doc-en-535898, doc-...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>[doc-en-701524, doc-en-56, doc-en-11186, doc-e...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"top_bm25.to_csv('/kaggle/working/submission.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del corpus\ndel bm25_model","metadata":{"execution":{"iopub.status.busy":"2024-11-08T17:44:55.776943Z","iopub.execute_input":"2024-11-08T17:44:55.777250Z","iopub.status.idle":"2024-11-08T17:45:05.055830Z","shell.execute_reply.started":"2024-11-08T17:44:55.777220Z","shell.execute_reply":"2024-11-08T17:45:05.054909Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## Dense retrieval","metadata":{}},{"cell_type":"code","source":"# %%time\n\n# # Load the document embeddings\n# with open('/kaggle/input/m3-embedding-of-512-chunks/chunk_embedding.pkl', 'rb') as f:\n#     chunk_embedding_dict = pickle.load(f)\n    \n# chunk_ids = list(chunk_embedding_dict.keys())\n# chunk_embeddings = np.array([chunk_embedding_dict[chunk_id] for chunk_id in chunk_ids]).astype('float32')","metadata":{"execution":{"iopub.status.busy":"2024-11-08T17:45:05.057430Z","iopub.execute_input":"2024-11-08T17:45:05.057994Z","iopub.status.idle":"2024-11-08T17:45:41.832987Z","shell.execute_reply.started":"2024-11-08T17:45:05.057964Z","shell.execute_reply":"2024-11-08T17:45:41.832086Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"CPU times: user 12.5 s, sys: 4.05 s, total: 16.6 s\nWall time: 36.8 s\n","output_type":"stream"}]},{"cell_type":"code","source":"# # Load the model\n# model = FlagModel('BAAI/bge-m3',\n#                   query_instruction_for_retrieval=\"Represent this sentence for searching relevant passages:\",\n#                   use_fp16=True)\n\n# # Embed the test queries\n# queries = test_df['query'].tolist()\n# query_ids = test_df['id'].tolist()\n# query_embeddings = model.encode(queries).astype('float32')","metadata":{"execution":{"iopub.status.busy":"2024-11-08T17:45:41.834034Z","iopub.execute_input":"2024-11-08T17:45:41.834289Z","iopub.status.idle":"2024-11-08T17:51:25.487124Z","shell.execute_reply.started":"2024-11-08T17:45:41.834263Z","shell.execute_reply":"2024-11-08T17:51:25.486181Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\nInference Embeddings: 100%|██████████| 8/8 [02:33<00:00, 19.14s/it] \n","output_type":"stream"}]},{"cell_type":"code","source":"# %%time\n\n# k = 100  # Number of nearest neighbors to retrieve\n\n# # Normalize the corpus matrix\n# faiss.normalize_L2(chunk_embeddings)\n\n# # Initialize a FAISS index\n# d = chunk_embeddings.shape[1]  # Dimensionality of embeddings\n# index = faiss.IndexFlatIP(d)  # IP = Inner Product, effectively cosine similarity after normalization\n# index.add(chunk_embeddings)  # Add document embeddings to the FAISS index\n\n# # Normalize query embeddings\n# faiss.normalize_L2(query_embeddings)\n\n# # Perform the search and retrieve top 100 results\n# distances, indices = index.search(query_embeddings, k)","metadata":{"execution":{"iopub.status.busy":"2024-11-08T17:51:25.488405Z","iopub.execute_input":"2024-11-08T17:51:25.488719Z","iopub.status.idle":"2024-11-08T17:53:12.694426Z","shell.execute_reply.started":"2024-11-08T17:51:25.488687Z","shell.execute_reply":"2024-11-08T17:53:12.693573Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"CPU times: user 25min 31s, sys: 5.97 s, total: 25min 37s\nWall time: 1min 47s\n","output_type":"stream"}]},{"cell_type":"code","source":"# # Map the indices back to document IDs\n# top_k_chunks = {\n#     query_id: [chunk_ids[idx] for idx in indices[i]]\n#     for i, query_id in enumerate(query_ids)\n# }\n\n# # Optional: Convert results to a DataFrame for easier access\n# top_k_chunks_df = pd.DataFrame({\n#     'id': query_ids,\n#     'chunkids': [top_k_chunks[qid] for qid in query_ids]\n# })\n\n# # Display the top results\n# top_k_chunks_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-11-08T17:53:12.695639Z","iopub.execute_input":"2024-11-08T17:53:12.695909Z","iopub.status.idle":"2024-11-08T17:53:12.809427Z","shell.execute_reply.started":"2024-11-08T17:53:12.695881Z","shell.execute_reply":"2024-11-08T17:53:12.808644Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"   id                                           chunkids\n0   0  [doc-en-0_chunk_13, doc-en-0_chunk_8, doc-en-1...\n1   1  [doc-en-16_chunk_57, doc-en-16_chunk_56, doc-e...\n2   2  [doc-it-14111_chunk_5, doc-en-32_chunk_7, doc-...\n3   3  [doc-en-563220_chunk_1, doc-en-44779_chunk_1, ...\n4   4  [doc-en-772504_chunk_0, doc-en-56_chunk_13, do...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>chunkids</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>[doc-en-0_chunk_13, doc-en-0_chunk_8, doc-en-1...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>[doc-en-16_chunk_57, doc-en-16_chunk_56, doc-e...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>[doc-it-14111_chunk_5, doc-en-32_chunk_7, doc-...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>[doc-en-563220_chunk_1, doc-en-44779_chunk_1, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>[doc-en-772504_chunk_0, doc-en-56_chunk_13, do...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# def rank_and_aggregate(df, aggregate_num=20):\n#     results = {}\n\n#     # Loop through each query's candidates\n#     for _, row in df.iterrows():\n#         doc_scores = defaultdict(float)\n#         query_id = row['id']\n#         chunks = row['chunkids']\n\n#         # Weight each chunk based on its position in the list (higher rank -> higher weight)\n#         for rank, chunk_id in enumerate(chunks, start=1):\n#             # Extract the document ID part (everything before \"_chunk\")\n#             doc_id = \"_\".join(chunk_id.split(\"_\")[:-2])\n#             # Calculate weight, for example, inversely proportional to the rank\n#             score = 1 / rank  # Adjust the weighting function if needed\n\n#             # Aggregate scores for each document\n#             doc_scores[doc_id] += score\n\n#         # Get the top 10 documents based on cumulative scores\n#         top_docs = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)[:aggregate_num]\n#         results[query_id] = [doc for doc, score in top_docs]\n\n#     # Convert results to DataFrame for easier access\n#     top_results_df = pd.DataFrame(list(results.items()), columns=['id', 'docids'])\n#     return top_results_df\n\n# # Apply the function\n# top_dense = rank_and_aggregate(top_k_chunks_df)\n# top_dense.head()  # Display the top results","metadata":{"execution":{"iopub.status.busy":"2024-11-08T17:53:12.810534Z","iopub.execute_input":"2024-11-08T17:53:12.810834Z","iopub.status.idle":"2024-11-08T17:53:13.139796Z","shell.execute_reply.started":"2024-11-08T17:53:12.810805Z","shell.execute_reply":"2024-11-08T17:53:13.139041Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"   id                                             docids\n0   0  [doc-en-0, doc-en-14117, doc-de-14895, doc-en-...\n1   1  [doc-en-16, doc-en-768850, doc-es-13327, doc-e...\n2   2  [doc-en-32, doc-it-14111, doc-en-659327, doc-e...\n3   3  [doc-en-563220, doc-en-751326, doc-en-44779, d...\n4   4  [doc-en-772504, doc-en-56, doc-en-810421, doc-...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>docids</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>[doc-en-0, doc-en-14117, doc-de-14895, doc-en-...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>[doc-en-16, doc-en-768850, doc-es-13327, doc-e...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>[doc-en-32, doc-it-14111, doc-en-659327, doc-e...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>[doc-en-563220, doc-en-751326, doc-en-44779, d...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>[doc-en-772504, doc-en-56, doc-en-810421, doc-...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Weighted combine","metadata":{}},{"cell_type":"code","source":"# def aggregate_rankings(bm25_results, dense_results, top_k=10):\n#     top_combined_results = []\n\n#     # Assign scores based on rank positions for BM25 results\n#     for i in range(len(bm25_results)):\n#         combined_scores = defaultdict(float)\n        \n#         bm25_docids = bm25_results.loc[i, 'docids']\n#         for rank, docid in enumerate(bm25_docids, start=1):\n#             combined_scores[docid] += 0.45 * 1 / rank  # Higher rank gives higher score\n\n#         dense_docids = dense_results.loc[i, 'docids']\n#         for rank, docid in enumerate(dense_docids, start=1):\n#             combined_scores[docid] += 0.55 * 1 / rank  # Higher rank gives higher score\n\n#         # Sort by combined score and retrieve the top k\n#         top_docs = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)[:top_k]\n#         top_docs = [item[0] for item in top_docs]\n#         top_combined_results.append(top_docs)\n#     return top_combined_results\n\n# # Get the top 10 results\n# top_10_combined_results = aggregate_rankings(top_bm25, top_dense, top_k=10)\n# top_final = pd.DataFrame({\n#     'id': query_ids,\n#     'docids': top_10_combined_results\n# })\n# top_final","metadata":{"execution":{"iopub.status.busy":"2024-11-08T18:11:20.796458Z","iopub.execute_input":"2024-11-08T18:11:20.797461Z","iopub.status.idle":"2024-11-08T18:11:20.908678Z","shell.execute_reply.started":"2024-11-08T18:11:20.797414Z","shell.execute_reply":"2024-11-08T18:11:20.907940Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"        id                                             docids\n0        0  [doc-en-0, doc-en-14117, doc-de-14895, doc-en-...\n1        1  [doc-en-16, doc-en-768850, doc-en-773190, doc-...\n2        2  [doc-en-32, doc-en-24920, doc-it-14111, doc-en...\n3        3  [doc-en-814022, doc-en-563220, doc-en-751326, ...\n4        4  [doc-en-772504, doc-en-56, doc-en-701524, doc-...\n...    ...                                                ...\n1995  1995  [doc-ar-8335, doc-en-1607, doc-ar-11184, doc-e...\n1996  1996  [doc-ar-8351, doc-ar-12175, doc-en-17143, doc-...\n1997  1997  [doc-ar-7659, doc-en-695959, doc-ar-2382, doc-...\n1998  1998  [doc-ar-8394, doc-ar-59, doc-ar-5176, doc-en-7...\n1999  1999  [doc-ar-1810, doc-it-15534, doc-en-278827, doc...\n\n[2000 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>docids</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>[doc-en-0, doc-en-14117, doc-de-14895, doc-en-...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>[doc-en-16, doc-en-768850, doc-en-773190, doc-...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>[doc-en-32, doc-en-24920, doc-it-14111, doc-en...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>[doc-en-814022, doc-en-563220, doc-en-751326, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>[doc-en-772504, doc-en-56, doc-en-701524, doc-...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1995</th>\n      <td>1995</td>\n      <td>[doc-ar-8335, doc-en-1607, doc-ar-11184, doc-e...</td>\n    </tr>\n    <tr>\n      <th>1996</th>\n      <td>1996</td>\n      <td>[doc-ar-8351, doc-ar-12175, doc-en-17143, doc-...</td>\n    </tr>\n    <tr>\n      <th>1997</th>\n      <td>1997</td>\n      <td>[doc-ar-7659, doc-en-695959, doc-ar-2382, doc-...</td>\n    </tr>\n    <tr>\n      <th>1998</th>\n      <td>1998</td>\n      <td>[doc-ar-8394, doc-ar-59, doc-ar-5176, doc-en-7...</td>\n    </tr>\n    <tr>\n      <th>1999</th>\n      <td>1999</td>\n      <td>[doc-ar-1810, doc-it-15534, doc-en-278827, doc...</td>\n    </tr>\n  </tbody>\n</table>\n<p>2000 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# top_final.to_csv('/kaggle/working/submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-11-08T18:18:00.682732Z","iopub.execute_input":"2024-11-08T18:18:00.683815Z","iopub.status.idle":"2024-11-08T18:18:00.706185Z","shell.execute_reply.started":"2024-11-08T18:18:00.683772Z","shell.execute_reply":"2024-11-08T18:18:00.705482Z"},"trusted":true},"execution_count":43,"outputs":[]}]}