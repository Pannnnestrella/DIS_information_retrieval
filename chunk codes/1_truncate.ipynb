{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## truncate.ipynb\n",
    "This notebook implements the following functionality:\n",
    "1. Loads a multilingual corpus and splits long documents into smaller chunks using the `RecursiveCharacterTextSplitter` from LangChain.\n",
    "2. Generates unique chunk IDs for each split and maps them to the corresponding text.\n",
    "3. Saves the chunked document data as a JSON file for later use.\n",
    "\n",
    "### Output\n",
    "- A JSON file (`chunk_doc_<chunk_size>.json`) containing document chunks with unique IDs.\n",
    "- Visualizations or distributions of document lengths before splitting.\n",
    "\n",
    "### Notes\n",
    "- Adjust `chunk_size` and `chunk_overlap` parameters in the `recursive()` function to suit specific use case.\n",
    "- The default chunk size is set to 512 characters with a 0-character overlap.\n",
    "- The notebook is designed to handle large multilingual corpora efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-10-22T13:24:59.482694Z",
     "iopub.status.busy": "2024-10-22T13:24:59.482463Z",
     "iopub.status.idle": "2024-10-22T13:25:18.807585Z",
     "shell.execute_reply": "2024-10-22T13:25:18.806778Z",
     "shell.execute_reply.started": "2024-10-22T13:24:59.482668Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !pip install gensim\n",
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-10-21T20:31:28.176434Z",
     "iopub.status.busy": "2024-10-21T20:31:28.175112Z",
     "iopub.status.idle": "2024-10-21T20:31:28.221577Z",
     "shell.execute_reply": "2024-10-21T20:31:28.220846Z",
     "shell.execute_reply.started": "2024-10-21T20:31:28.176391Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\15163\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\15163\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import FastText\n",
    "from tqdm import tqdm\n",
    "\n",
    "# This splitter will try to keep the sentence structure and keep the whole words.\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-21T20:31:30.873171Z",
     "iopub.status.busy": "2024-10-21T20:31:30.872238Z",
     "iopub.status.idle": "2024-10-21T20:32:48.549053Z",
     "shell.execute_reply": "2024-10-21T20:32:48.548142Z",
     "shell.execute_reply.started": "2024-10-21T20:31:30.873130Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "corpus_path = '../dataset/corpus.json'\n",
    "\n",
    "with open(corpus_path, 'r') as f:\n",
    "    corpus = json.load(f)\n",
    "\n",
    "doc_ids = [doc['docid'] for doc in corpus]\n",
    "doc_texts = [doc['text'] for doc in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T19:15:41.312581Z",
     "iopub.status.busy": "2024-10-20T19:15:41.311333Z",
     "iopub.status.idle": "2024-10-20T19:16:36.964661Z",
     "shell.execute_reply": "2024-10-20T19:16:36.962858Z",
     "shell.execute_reply.started": "2024-10-20T19:15:41.312530Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# doc_lengths = {doc['docid']: len(doc['text'].split()) for doc in corpus}\n",
    "# lengths = list(doc_lengths.values())\n",
    "\n",
    "# # Plot the distribution of text lengths\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.hist(lengths, bins=100, edgecolor='black', alpha=0.7)\n",
    "# plt.title('Distribution of Document Lengths (in words)')\n",
    "# plt.xlabel('Document Length (Number of Words)')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive(chunk_size, chunk_overlap):\n",
    "    recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    return recursive_splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_id = doc_ids[0]\n",
    "doc_text = doc_texts[0]\n",
    "\n",
    "chunk_doc_index = {}\n",
    "chunks = recursive(512, 0).split_text(doc_text)\n",
    "for i, chunk in enumerate(chunks):\n",
    "    chunk_id = f\"{doc_id}_chunk_{i}\"  # Create a unique ID for each chunk\n",
    "    chunk_doc_index[chunk_id] = chunk # map the id with the String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-10-21T20:33:13.598706Z",
     "iopub.status.busy": "2024-10-21T20:33:13.597782Z",
     "iopub.status.idle": "2024-10-21T22:02:57.320825Z",
     "shell.execute_reply": "2024-10-21T22:02:57.320122Z",
     "shell.execute_reply.started": "2024-10-21T20:33:13.598667Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Documents: 100%|██████████| 268022/268022 [09:03<00:00, 492.99it/s] \n"
     ]
    }
   ],
   "source": [
    "chunk_size = 512\n",
    "\n",
    "# # Function to chunk the document into chunks of 128 tokens\n",
    "# def chunk_document(doc_text, chunk_size=128):\n",
    "#     tokens = word_tokenize(doc_text)\n",
    "#     # Split tokens into chunks of size chunk_size\n",
    "#     return [tokens[i:i+chunk_size] for i in range(0, len(tokens), chunk_size)]\n",
    "\n",
    "# Build the chunk-doc index: a dictionary where the key is a chunk and the value is the doc_id\n",
    "chunk_doc_index = {}\n",
    "for doc_id, doc_text in tqdm(zip(doc_ids, doc_texts), total=len(doc_ids), desc=\"Processing Documents\"):\n",
    "    chunks = recursive(chunk_size, 40).split_text(doc_text)\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk_id = f\"{doc_id}_chunk_{i}\"  # Create a unique ID for each chunk\n",
    "        chunk_doc_index[chunk_id] = chunk # map the id with the String\n",
    "\n",
    "# Show the first few chunks\n",
    "# list(chunk_doc_index.items())[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-21T22:03:10.997889Z",
     "iopub.status.busy": "2024-10-21T22:03:10.997556Z",
     "iopub.status.idle": "2024-10-21T22:09:07.028788Z",
     "shell.execute_reply": "2024-10-21T22:09:07.028013Z",
     "shell.execute_reply.started": "2024-10-21T22:03:10.997863Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Save as JSON file\n",
    "with open(f'./chunk_doc_{chunk_size}.json', 'w') as json_file:\n",
    "    json.dump(chunk_doc_index, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-21T22:12:56.228318Z",
     "iopub.status.busy": "2024-10-21T22:12:56.227976Z",
     "iopub.status.idle": "2024-10-21T22:19:20.198944Z",
     "shell.execute_reply": "2024-10-21T22:19:20.197898Z",
     "shell.execute_reply.started": "2024-10-21T22:12:56.228290Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 7s, sys: 16.6 s, total: 6min 23s\n",
      "Wall time: 6min 23s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "\n",
    "# tokenized_chunks = chunk_doc_index.values()\n",
    "\n",
    "# # Create the BM25 model\n",
    "# bm25 = BM25Okapi(tokenized_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-21T22:21:42.423668Z",
     "iopub.status.busy": "2024-10-21T22:21:42.422528Z",
     "iopub.status.idle": "2024-10-21T22:27:04.479324Z",
     "shell.execute_reply": "2024-10-21T22:27:04.478474Z",
     "shell.execute_reply.started": "2024-10-21T22:21:42.423626Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 42s, sys: 39.5 s, total: 5min 22s\n",
      "Wall time: 5min 22s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['doc-en-376830_chunk_1',\n",
       " 'doc-en-461662_chunk_0',\n",
       " 'doc-en-632778_chunk_5',\n",
       " 'doc-en-6846_chunk_9',\n",
       " 'doc-en-278325_chunk_3']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%time\n",
    "\n",
    "# # Function to retrieve top N chunks for a query\n",
    "# def bm25_retrieve(query, top_n=1000):\n",
    "#     tokenized_query = word_tokenize(query)\n",
    "#     scores = bm25.get_scores(tokenized_query)\n",
    "#     ranked_chunks = sorted(enumerate(scores), key=lambda x: x[1], reverse=True)\n",
    "#     top_chunks = [list(chunk_doc_index.keys())[idx] for idx, score in ranked_chunks[:top_n]]\n",
    "#     return top_chunks\n",
    "\n",
    "# # Example: Retrieve top 1000 chunks for a query\n",
    "# query = \"How to do efficient retrieval in large multilingual corpus for long texts\"\n",
    "# top_chunks = bm25_retrieve(query, top_n=1000)\n",
    "\n",
    "# # Output the top chunks\n",
    "# top_chunks[:5]"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "tpu1vmV38",
   "dataSources": [
    {
     "databundleVersionId": 9635715,
     "sourceId": 85316,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
