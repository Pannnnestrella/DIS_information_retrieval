{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1_spilt_json.ipynb\n",
    "\n",
    "This notebook provides a script to split a large JSON file into smaller chunks. The main tasks include:\n",
    "1. Reading a JSON file containing key-value pairs.\n",
    "2. Splitting the JSON data into a specified number of smaller files.\n",
    "3. Saving each chunk as a separate JSON file with a unique name.\n",
    "\n",
    "### Instructions\n",
    "- Update the `file_path` variable with the path to your large JSON file.\n",
    "- Specify the number of chunks in the `num_chunks` parameter of the `split_json_file` function.\n",
    "- Run the notebook to generate smaller JSON files.\n",
    "\n",
    "### Output\n",
    "- Multiple smaller JSON files (e.g., `chunk_doc_512_1.json`, `chunk_doc_512_2.json`, etc.), each containing approximately equal portions of the original data.\n",
    "\n",
    "### Notes\n",
    "- The script ensures that the last chunk contains any remaining data if the total number of records is not divisible by `num_chunks`.\n",
    "- The generated files are saved in the current working directory with descriptive names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def split_json_file(file_path, num_chunks=5):\n",
    "    # Read JSON files\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # Convert dictionary data into a list of (key, value)\n",
    "    items = list(data.items())\n",
    "    chunk_size = len(items) // num_chunks\n",
    "\n",
    "    for i in range(num_chunks):\n",
    "        # Calculate the start and end indexes of the current mini-file\n",
    "        start_idx = i * chunk_size\n",
    "        end_idx = start_idx + chunk_size if i < num_chunks - 1 else len(items)\n",
    "\n",
    "        # Data for the current small file\n",
    "        chunk_data = dict(items[start_idx:end_idx])\n",
    "\n",
    "        # Write small files\n",
    "        chunk_file_name = f\"chunk_doc_512_{i + 1}.json\"\n",
    "        with open(chunk_file_name, 'w', encoding='utf-8') as chunk_file:\n",
    "            json.dump(chunk_data, chunk_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "        print(f\"Saved {chunk_file_name} with {len(chunk_data)} records.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path\n",
    "file_path = './chunk_512/chunk_doc_512.json'\n",
    "split_json_file(file_path, num_chunks=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
